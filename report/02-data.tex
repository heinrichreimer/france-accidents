\section{Data}
\label{data}
\begin{table}
    \caption{Files with accident data, released by the French government each year~(denoted by \textit{YYYY}), with description of its contents.}
    \label{table-files}
    \begin{tabularx}{\linewidth}{lX}
        \toprule
        \textbf{File Name} & \textbf{Contents} \\
        \midrule
        \textit{caracteristiques-YYYY.csv} & accident characteristics, time, place, etc. \\
        \textit{lieux-YYYY.csv} & accident location, road numbers, road characteristics, lanes etc. \\
        \textit{usagers-YYYY.csv} & persons involved, sex, birth, equipment, etc. \\
        \textit{vehicules-YYYY.csv} & vehicle type, maneuver, hit obstacles \\
        \textit{vehicules-immatricules-baac-YYYY.csv} & not used in this report \\
        \bottomrule
    \end{tabularx}
\end{table}
The French government publicly releases a dataset\footnote{\url{https://data.gouv.fr/en/datasets/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2019/}} of  metadata on road accidents that occured between~2005 and~2020. This dataset is released yearly and consist of 5~files per year in CSV format~(cf. Table~\ref{table-files}). Excluding one of the files\footnote{The \textit{vehicules-immatricules-baac-YYYY.csv} files are only available from 2010 to 2020.}, we download a total of 64~files for the 16~years considered for our visualizations. As shown in  Table~\ref{table-files}, the dataset feature a variety of dimensions. Many categorical dimensions describe characteristics of the accidents, vehicles, and persons involved. Using the categories we can filter or partition the dataset across multiple dimensions, in order to focus on specific characteristics in the dataset. The involved persons are also characterized regarding injuries and deadliness of the accident, which allows us to measure not only the number of accidents but also how severe each accident was. Similarly, each vehicle and each person involved in the accident is listed, so that we can count how much impact the accident likely had on the society. Crucially for measuring trends across years or months, each accident in the dataset is annotated with a timestamp. We therefore argue that the provided dataset is well suited to answer questions mentioned in Section~\ref{introduction}. Unfortunately, accidents, locations, vehicles, and persons are distributed in separate files. Hence, in our data preprocessing pipeline, we associate locations, vehicles, and persons to their matching accident.

After first attempts to parse and merge the CSV files directly inside the Elm application we decided to implement the data preprocessing in Python instead, as the type safety in Elm makes it complicated to correct errors in the goverment open data files. For example, CSV files from some years use different separators and even different file encodings. Also some fields are named differently or use different number formats. In Python such errors are easier to debug and work around. By preprocessing the CSV files we also group persons by their vehicle and vehicles by the accident.Our Python preprocessing is structured in three parsers, for accidents, vehicles, and persons. After parsing, the three lists are grouped and exported. A smaller sample is extracted due to memory constraints when loading the dataset in the web browser.

\paragraph{Parsers}
\begin{listing}
    \lstinputlisting[linerange={17-17,32-36,43-45,48-48,59-59,74-77,110-111},language=Python]{../preprocessing/parse/person.py}
    \caption{Parser module for the CSV file containing person information. Function and constructor abbreviated.}
    \label{listing-data-parser}
\end{listing}
For each of the four data types, characteristics, locations, vehicles, and persons, we implement a parser module as illustrated in Listing~\ref{listing-data-parser}. The parser modules are responsible for opening the corresponding dataset file for the specified year and then reading the CSV records with the correct file encoding and separator. From the CSV records we then extract Python named tuples to improve type-safety for the JSON export. All columns are stripped from surounding whitespace or illegal special charcters~(e.g., the non-breaking space). For categorical columns~(cf. Listing~\ref{listing-data-parser}), we filter out invalid values and then map the values to Python \lstinline[language=Python]{Enum} classes containing the value coding from the French government's dataset description. Floating point values are parsed after correcting the decimal separator. We also split and parse lists of values contained in the CSV columns~(e.g., for the \lstinline[language=]{secu} column specifying safety equipment). For some columns, we also remove prefixes and suffixes~(e.g., for the upstream terminal~\lstinline[language=]{pr1}). The upstream terminal data is particularly difficult to parse because for some years the \lstinline[language=]{pr}~and \lstinline[language=]{pr1}~columns are swapped and need to be corrected. Each parser returns a list of instances of its corresponding named tuple together with the accident ID, vehicle ID, and person number if applicable.

\paragraph{Accident Grouping}
After parsing records of accidents, vehicles, and persons we match the vehicle and person records with the accidents. This is done by grouping persons by their vehicle ID and then associating each group of persons with the corresponding vehicle. Similarly, vehicles~(together with their associated persons) are then grouped by accident ID and then associating the vehicle groups with the matching accident record. This mapping from three different lists of records into a single list of reords, containing lists of lists, makes parsing a single accidents easier in the Elm application. 

\paragraph{Export and Sampling}
\label{export}
After preprocessing the data, we bundle the accident dataset as static resource, writing one JSON record per line to the exported JSONL file. For the conversion from Python's named tuple types that we modelled for each of the record types~(i.e., accident, vehicle, and person) to JSON, we convert the named tuples to Python \lstinline[language=Python]{Dict} instances which then can be converted to a JSON string using the built in \lstinline[language=Python]{json.dumps} function. Because of the large size of the aggregated dataset~(1065053~accidents/lines or 2~GB disk size), we also derive a smaller subset of the dataset by uniformly sampling 10\,000 random accidents from the whole dataset~(19~MB disk space). This way we extract a dataset that can be used with regular web browsers on common consumer hardware. To account for sampling biases, we test the visualizations with 3~different samples and find no major visual differences across the samples. We therefore assume that the sample dataset is able to sufficently represent the whole dataset.
