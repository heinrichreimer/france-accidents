\section{Data}
\label{data}
\begin{table}
    \caption{Files with accident data, released by the French government each year~(denoted by \textit{YYYY}), with description of its contents.}
    \label{table-files}
    \begin{tabularx}{\linewidth}{lX}
        \toprule
        \textbf{File Name} & \textbf{Contents} \\
        \midrule
        \textit{caracteristiques-YYYY.csv} & accident characteristics, time, place, etc. \\
        \textit{lieux-YYYY.csv} & accident location, road numbers, road characteristics, lanes etc. \\
        \textit{usagers-YYYY.csv} & persons involved, sex, birth, equipment, etc. \\
        \textit{vehicules-YYYY.csv} & vehicle type, maneuver, hit obstacles \\
        \textit{vehicules-immatricules-baac-YYYY.csv} & not used in this report \\
        \bottomrule
    \end{tabularx}
\end{table}
The French government publicly releases a dataset of  metadata on road accidents.\footnote{\url{https://data.gouv.fr/en/datasets/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2019/}}
This dataset is released yearly and consist of 4~files per year in CSV format. As shown in  Table~\ref{table-files}, the dataset feature a variety of dimensions. Many categorical dimensions describe characteristics of the accidents, vehicles, and persons involved.
Using the categories we can filter or partition the dataset across multiple dimensions, in order to focus on specific characteristics in the dataset.
The involved persons are also characterized regarding injuries and deadliness of the accident, which allows us to measure not only the number of accidents but also how severe each accident was. Similarly, each vehicle and each person involved in the accident is listed, so that we can count how much impact the accident likely had on the society. Crucially for measuring trends across years or months, each accident in the dataset is annotated with a timestamp. We therefore argue that the provided dataset is well suited to answer questions mentioned in Section~\ref{introduction}.

After first attempts to parse and merge the CSV files directly inside the Elm application we decided to implement the data preprocessing in Python instead, as the type safety in Elm makes it complicated to correct errors in the goverment open data files. For example, CSV files from some years use different separators and even different file encodings. Also some fields are named differently or use different number formats. In Python such errors are easier to debug and work around. By preprocessing the CSV files we also group persons by their vehicle and vehicles by the accident, in order to make parsing a single accident record easier. Our Python preprocessing is structured in three parsers, for accidents, vehicles, and persons. After parsing, the three lists are grouped and exported. A smaller sample is extracted due to memory constraints when loading the dataset in the web browser.

\paragraph{Accident Parser}

\paragraph{Accident Parser}

\paragraph{Accident Parser}

\paragraph{Accident Grouping}

% Preprocessing with Python
\todo{Beschreiben Sie vorhandenen Daten. Gehen Sie kritisch darauf ein, inwieweit sich die Daten für die Bearbeitung der Fragestellungen und dem Erreichen von Lösungen für die oben beschriebene Zielgruppen eignen. Haben sie die Daten sinnvoll mit weiteren Datenquellen ergänzt? Wenn ja, wie?
Erklären Sie die technische Bereitstellung der Daten.
Wie sind die Daten zugänglich? Welche Formate werden genutzt. Gibt es Besonderheiten beim Lesen der Formate?
Beschreiben Sie die Datenvorverarbeitung.
Welche Datenvorverarbeitungsschritte sind notwendig? Beschreiben Sie die einzelnen Schritte und begründen Sie sie, zum Beispiel warum werden manche Daten weggelassen, über welche Mengen werden Durchschnitte berechnet, warum sind die so berechneten Werte aussagekräftiger als andere Werte. Wenn möglich sollen sie die Datenvorverarbeitung in Elm programmieren, sodass ihre Anwendung auf eine Änderung der Rohdaten reagieren kann.}


\paragraph{Re-formatting as JSON}


\paragraph{Export and Sampling}

After preprocessing the data, we bundle the accident dataset as static resource, writing one JSON record per line to the exported JSONL file. Because of the large size of the aggregated dataset~(1065053~accidents/lines or 2~GB disk size), we also derive a smaller subset of the dataset by uniformly sampling 10\,000 random accidents from the whole dataset~(19~MB disk space). This way we extract a dataset that can be used with regular web browsers on common consumer hardware.
To account for sampling biases, we test the visualizations with 3~different samples and find no major visual differences across the samples. We therefore assume that the sample is able to sufficently represent the whole dataset. In our visualization application implementation in the Elm language, we then model and parse the exported JSON data structures~(cf.~Section~\ref{implementation}).
